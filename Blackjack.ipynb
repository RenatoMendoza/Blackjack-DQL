{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Starting Training ===\n",
      "Episode: 0/500, ε: 1.000\n",
      "Episode: 100/500, ε: 0.615\n",
      "Episode: 200/500, ε: 0.353\n",
      "Episode: 300/500, ε: 0.197\n",
      "Episode: 400/500, ε: 0.108\n",
      "\n",
      "=== Evaluating Agent ===\n",
      "Win rate: 43.10% over 1000 episodes\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "from tensorflow.keras.models import Sequential, clone_model\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "class DQLAgent:\n",
    "    \"\"\"Deep Q-Learning Agent with experience replay and target network\"\"\"\n",
    "    \n",
    "    def __init__(self, state_size, action_size):\n",
    "        \"\"\"\n",
    "        Initialize DQL Agent with key parameters\n",
    "        \n",
    "        Args:\n",
    "            state_size (int): Dimension of state space\n",
    "            action_size (int): Number of possible actions\n",
    "        \"\"\"\n",
    "        # Environment parameters\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        \n",
    "        # Experience replay buffer (stores SARS' tuples)\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        \n",
    "        # Q-Learning parameters\n",
    "        self.gamma = 0.95    # Discount factor for future rewards\n",
    "        self.epsilon = 1.0   # Initial exploration rate\n",
    "        self.epsilon_min = 0.01  # Minimum exploration probability\n",
    "        self.epsilon_decay = 0.995  # Decay rate for exploration prob\n",
    "        self.learning_rate = 0.001  # Neural network learning rate\n",
    "        \n",
    "        # Neural networks (Q-network and target network)\n",
    "        self.model = self._build_model()\n",
    "        self.target_model = self._build_model()\n",
    "        self.update_target_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        \"\"\"Build neural network architecture for Q-value approximation\"\"\"\n",
    "        model = Sequential([\n",
    "            Input(shape=(self.state_size,)),  # Input layer for state\n",
    "            Dense(24, activation='relu'),     # Hidden layer with 24 units\n",
    "            Dense(24, activation='relu'),     # Second hidden layer\n",
    "            Dense(self.action_size, activation='linear')  # Output layer (Q-values)\n",
    "        ])\n",
    "        \n",
    "        model.compile(\n",
    "            loss='mse',  # Mean squared error for Q-value regression\n",
    "            optimizer=Adam(learning_rate=self.learning_rate)\n",
    "        )\n",
    "        return model\n",
    "\n",
    "    def update_target_model(self):\n",
    "        \"\"\"Sync target network weights with main network weights\"\"\"\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    def preprocess_state(self, state):\n",
    "        \"\"\"\n",
    "        Normalize state components for neural network input\n",
    "        \n",
    "        Args:\n",
    "            state (tuple): Raw state from environment (player_sum, dealer_card, usable_ace)\n",
    "            \n",
    "        Returns:\n",
    "            np.array: Normalized state vector\n",
    "        \"\"\"\n",
    "        player_sum, dealer_card, usable_ace = state\n",
    "        return np.array([\n",
    "            (player_sum - 2) / 29,       # Normalize player sum (2-31) to [0,1]\n",
    "            (dealer_card - 1) / 9,       # Normalize dealer card (1-10) to [0,1]\n",
    "            usable_ace                   # Binary value remains 0 or 1\n",
    "        ])\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        Store experience in replay memory with preprocessing\n",
    "        \n",
    "        Args:\n",
    "            state (tuple): Current state\n",
    "            action (int): Taken action\n",
    "            reward (float): Received reward\n",
    "            next_state (tuple): Next state\n",
    "            done (bool): Episode completion flag\n",
    "        \"\"\"\n",
    "        state = self.preprocess_state(state)\n",
    "        next_state = self.preprocess_state(next_state)\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        \"\"\"\n",
    "        Select action using ε-greedy policy\n",
    "        \n",
    "        Args:\n",
    "            state (tuple): Current environment state\n",
    "            \n",
    "        Returns:\n",
    "            int: Selected action (0 = stand, 1 = hit)\n",
    "        \"\"\"\n",
    "        # Exploration: random action\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        \n",
    "        # Exploitation: best predicted action\n",
    "        state = self.preprocess_state(state)\n",
    "        q_values = self.model.predict(np.expand_dims(state, axis=0), verbose=0)\n",
    "        return np.argmax(q_values[0])\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        \"\"\"\n",
    "        Train neural network using experiences from replay buffer\n",
    "        \n",
    "        Args:\n",
    "            batch_size (int): Number of experiences to sample from memory\n",
    "        \"\"\"\n",
    "        if len(self.memory) < batch_size:\n",
    "            return  # Not enough experiences to train\n",
    "        \n",
    "        # Sample random minibatch from experience replay buffer\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        \n",
    "        # Unpack batch components\n",
    "        states = np.array([t[0] for t in minibatch])\n",
    "        actions = np.array([t[1] for t in minibatch])\n",
    "        rewards = np.array([t[2] for t in minibatch])\n",
    "        next_states = np.array([t[3] for t in minibatch])\n",
    "        dones = np.array([t[4] for t in minibatch])\n",
    "\n",
    "        # Predict Q-values for current and next states\n",
    "        current_q = self.model.predict(states, verbose=0)\n",
    "        future_q = self.target_model.predict(next_states, verbose=0)\n",
    "\n",
    "        # Update Q-values using Bellman equation\n",
    "        for i in range(len(minibatch)):\n",
    "            if dones[i]:\n",
    "                # Terminal state: Q-value = immediate reward\n",
    "                current_q[i][actions[i]] = rewards[i]\n",
    "            else:\n",
    "                # Non-terminal: Q-value = reward + γ*max_future_q\n",
    "                current_q[i][actions[i]] = rewards[i] + self.gamma * np.max(future_q[i])\n",
    "\n",
    "        # Train network with updated Q-values\n",
    "        self.model.fit(states, current_q, epochs=1, verbose=0)\n",
    "        \n",
    "        # Decay exploration rate\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "def train_agent(episodes=1000, batch_size=32, update_target_freq=50):\n",
    "    \"\"\"\n",
    "    Train DQL agent in Blackjack environment\n",
    "    \n",
    "    Args:\n",
    "        episodes (int): Number of training episodes\n",
    "        batch_size (int): Experience replay batch size\n",
    "        update_target_freq (int): Target network update frequency\n",
    "        \n",
    "    Returns:\n",
    "        DQLAgent: Trained agent\n",
    "    \"\"\"\n",
    "    env = gym.make('Blackjack-v1')\n",
    "    state_size = 3  # (player_sum, dealer_card, usable_ace)\n",
    "    action_size = env.action_space.n  # 2 actions (stand, hit)\n",
    "    \n",
    "    agent = DQLAgent(state_size, action_size)\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            # Agent-environment interaction loop\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            # Store experience and train\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            \n",
    "            # Start training when enough experiences are collected\n",
    "            if len(agent.memory) > batch_size:\n",
    "                agent.replay(batch_size)\n",
    "            \n",
    "            state = next_state\n",
    "        \n",
    "        # Periodic target network updates\n",
    "        if episode % update_target_freq == 0:\n",
    "            agent.update_target_model()\n",
    "        \n",
    "        # Progress reporting\n",
    "        if episode % 100 == 0:\n",
    "            print(f\"Episode: {episode}/{episodes}, ε: {agent.epsilon:.3f}\")\n",
    "    \n",
    "    return agent\n",
    "\n",
    "def evaluate_agent(agent, episodes=1000):\n",
    "    \"\"\"\n",
    "    Evaluate agent performance in Blackjack environment\n",
    "    \n",
    "    Args:\n",
    "        agent (DQLAgent): Trained agent to evaluate\n",
    "        episodes (int): Number of evaluation episodes\n",
    "        \n",
    "    Returns:\n",
    "        float: Win rate percentage\n",
    "    \"\"\"\n",
    "    env = gym.make('Blackjack-v1')\n",
    "    wins = 0\n",
    "    \n",
    "    for _ in range(episodes):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            state = next_state\n",
    "            \n",
    "        if reward == 1:\n",
    "            wins += 1\n",
    "    \n",
    "    win_rate = (wins / episodes) * 100\n",
    "    print(f\"Win rate: {win_rate:.2f}% over {episodes} episodes\")\n",
    "    return win_rate\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Training and evaluation pipeline\n",
    "    print(\"=== Starting Training ===\")\n",
    "    trained_agent = train_agent(episodes=500)  # Increased training episodes\n",
    "    \n",
    "    print(\"\\n=== Evaluating Agent ===\")\n",
    "    evaluate_agent(trained_agent)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
